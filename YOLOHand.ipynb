{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e04e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "net = cv2.dnn.readNet(\"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\Hand-Gestures-Recognition-master\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom_last.weights\", \"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\Hand-Gestures-Recognition-master\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom.cfg\") \n",
    "classes = list()\n",
    "f= open(\"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\Hand-Gestures-Recognition-master\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\obj.names\", \"r\") # read .names file\n",
    "for line in f.readlines(): # loop each line in file\n",
    "    classes.append(line.strip())  # split removes extra spaces and append the rest data into the list\n",
    "layer_names = net.getLayerNames() # get layes names\n",
    "layer=list() # init layes list\n",
    "for i in net.getUnconnectedOutLayers():\n",
    "    # we are getting the output layer to get detection of the object\n",
    "    layer.append(layer_names[i - 1])\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "#provide video path in string or 0== internal webcam 2== external webcam\n",
    "cap = cv2.VideoCapture(0) # read input from default. 0 is for webcam 1 is for video and 2 is for exteral webcam \n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "while(cap.isOpened()):\n",
    "    # get video Frame by Frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        img=frame#set img to frame.\n",
    "        height, width,_ = img.shape #this will get image height and width \n",
    "        # create input blob to detect image\n",
    "        #0.00392 is the scale factor\n",
    "        #416,416 is the size we are passing to the algorithm\n",
    "        # it basically prepares the input image to run through the deep neural network.\n",
    "        #openCV works with BGR format  \n",
    "        #so blob has 3 channels BGR \n",
    "        #416 * 416 image\n",
    "        blobObject = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False) \n",
    "        net.setInput(blobObject) \n",
    "        outputs = net.forward(layer)#get next layer\n",
    "        #this basically helps us to extract the bounding box \n",
    "        class_ids = list() #helps detect class label \n",
    "        confidences = list() #assurity level of what an object is lower the value means that the system couldn't detect it 0.5 is threshold value \n",
    "        boxes = list() #bounding box \n",
    "        #displaying information on the screen \n",
    "        for out in outputs:\n",
    "            for detection in out: \n",
    "                scores = detection[5:]  #detection[5:] returns probability of each score \n",
    "                class_id = np.argmax(scores) #class id is associated with class list name which tells about the object\n",
    "                confidence = scores[class_id] \n",
    "                if confidence > 0.5: #tresh >0.5 it goes from 0 to 1\n",
    "                   #object detected\n",
    "                   #get center \n",
    "                   #when object will be detected it will mark a O on it \n",
    "                    center_x = int(detection[0] * width) #returns center x \n",
    "                    center_y = int(detection[1] * height) #return center y\n",
    "                    #get height and width \n",
    "                    w = int(detection[2] * width) #2 gives wudth\n",
    "                    h = int(detection[3] * height) #3 returns height \n",
    "                    # get points for rectangle\n",
    "                    x = int(center_x - w / 2) #top left x\n",
    "                    y = int(center_y - h / 2) #top left y \n",
    "                    #combine makes top left object\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        dec = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)#number of dectected objects in frame\n",
    "        for i in range(len(boxes)):\n",
    "            if i in dec:\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])#get label\n",
    "                color = colors[i]#each classs has specific color defined above\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)#create rectangle\n",
    "                cv2.putText(img, label, (10,50), cv2.FONT_ITALIC, 2, color, 3)\n",
    "        cv2.imshow(\"HAND GESTURE\", img)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395186dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5b6e341ab974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;31m# Close the camera if 'q' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# Open Camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while capture.isOpened():\n",
    "\n",
    "    # Capture frames from the camera\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Get hand data from the rectangle sub window\n",
    "    cv2.rectangle(frame, (100, 100), (300, 300), (0, 255, 0), 0)\n",
    "    crop_image = frame[100:300, 100:300]\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blur = cv2.GaussianBlur(crop_image, (3, 3), 0)\n",
    "\n",
    "    # Change color-space from BGR -> HSV\n",
    "    hsv = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a binary image with where white will be skin colors and rest is black\n",
    "    mask2 = cv2.inRange(hsv, np.array([2, 0, 0]), np.array([20, 255, 255]))\n",
    "\n",
    "    # Kernel for morphological transformation\n",
    "    kernel = np.ones((5, 5))\n",
    "\n",
    "    # Apply morphological transformations to filter out the background noise\n",
    "    dilation = cv2.dilate(mask2, kernel, iterations=1)\n",
    "    erosion = cv2.erode(dilation, kernel, iterations=1)\n",
    "\n",
    "    # Apply Gaussian Blur and Threshold\n",
    "    filtered = cv2.GaussianBlur(erosion, (3, 3), 0)\n",
    "    ret, thresh = cv2.threshold(filtered, 127, 255, 0)\n",
    "\n",
    "    # Show threshold image\n",
    "    cv2.imshow(\"Thresholded\", thresh)\n",
    "\n",
    "    # Find contours\n",
    "    image = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    try:\n",
    "        # Find contour with maximum area\n",
    "        contour = max(contours, key=lambda x: cv2.contourArea(x))\n",
    "\n",
    "        # Create bounding rectangle around the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(crop_image, (x, y), (x + w, y + h), (0, 0, 255), 0)\n",
    "\n",
    "        # Find convex hull\n",
    "        hull = cv2.convexHull(contour)\n",
    "\n",
    "        # Draw contour\n",
    "        drawing = np.zeros(crop_image.shape, np.uint8)\n",
    "        cv2.drawContours(drawing, [contour], -1, (0, 255, 0), 0)\n",
    "        cv2.drawContours(drawing, [hull], -1, (0, 0, 255), 0)\n",
    "\n",
    "        # Find convexity defects\n",
    "        hull = cv2.convexHull(contour, returnPoints=False)\n",
    "        defects = cv2.convexityDefects(contour, hull)\n",
    "\n",
    "        # Use cosine rule to find angle of the far point from the start and end point i.e. the convex points (the finger\n",
    "        # tips) for all defects\n",
    "        count_defects = 0\n",
    "\n",
    "        for i in range(defects.shape[0]):\n",
    "            s, e, f, d = defects[i, 0]\n",
    "            start = tuple(contour[s][0])\n",
    "            end = tuple(contour[e][0])\n",
    "            far = tuple(contour[f][0])\n",
    "\n",
    "            a = math.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "            b = math.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "            c = math.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "            angle = (math.acos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c)) * 180) / 3.14\n",
    "\n",
    "            # if angle > 90 draw a circle at the far point\n",
    "            if angle <= 90:\n",
    "                count_defects += 1\n",
    "                cv2.circle(crop_image, far, 1, [0, 0, 255], -1)\n",
    "\n",
    "            cv2.line(crop_image, start, end, [0, 255, 0], 2)\n",
    "\n",
    "        # Print number of fingers\n",
    "        if count_defects == 0:\n",
    "            cv2.putText(frame, \"ONE\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2,(0,0,255),2)\n",
    "        elif count_defects == 1:\n",
    "            cv2.putText(frame, \"TWO\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2,(0,0,255), 2)\n",
    "        elif count_defects == 2:\n",
    "            cv2.putText(frame, \"THREE\", (5, 50), cv2.FONT_HERSHEY_SIMPLEX, 2,(0,0,255), 2)\n",
    "        elif count_defects == 3:\n",
    "            cv2.putText(frame, \"FOUR\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2,(0,0,255), 2)\n",
    "        elif count_defects == 4:\n",
    "            cv2.putText(frame, \"FIVE\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 2,(0,0,255), 2)\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Show required images\n",
    "    cv2.imshow(\"Gesture\", frame)\n",
    "    all_image = np.hstack((drawing, crop_image))\n",
    "    cv2.imshow('Contours', all_image)\n",
    "\n",
    "    # Close the camera if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a145456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.6.0.66-cp36-abi3-win_amd64.whl (35.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\new folder\\lib\\site-packages (from opencv-python) (1.20.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228318aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:217: error: (-212:Parsing error) Failed to parse NetParameter file: D:\\downloads\\Hand-Gestures-Recognition\\Hand-Gestures-Recognition-master\\Yolo\\yolov3_custom_last.weights in function 'cv::dnn::dnn4_v20221220::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7019f6fc8e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\\\downloads\\\\Hand-Gestures-Recognition\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom_last.weights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"D:\\\\downloads\\\\Hand-Gestures-Recognition\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#net = cv2.dnn.readNet(\"yolov3_custom_last.weights\", \"yolov3_custom.cfg\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:217: error: (-212:Parsing error) Failed to parse NetParameter file: D:\\downloads\\Hand-Gestures-Recognition\\Hand-Gestures-Recognition-master\\Yolo\\yolov3_custom_last.weights in function 'cv::dnn::dnn4_v20221220::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "net = cv2.dnn.readNet(\"D:\\\\downloads\\\\Hand-Gestures-Recognition\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom_last.weights\", \"D:\\\\downloads\\\\Hand-Gestures-Recognition\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom.cfg\") \n",
    "#net = cv2.dnn.readNet(\"yolov3_custom_last.weights\", \"yolov3_custom.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"D:\\\\downloads\\\\Hand-Gestures-Recognition\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\obj.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    height, width, channels = frame.shape\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            color = colors[class_ids[i]]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    cv2.imshow(\"Hand Gesture Recognition\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199ad812",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f99f6af4d869>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Get the names of the output layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlayer_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0moutput_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Set the input image and scale it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f99f6af4d869>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Get the names of the output layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlayer_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0moutput_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Set the input image and scale it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained YOLO model\n",
    "net = cv2.dnn.readNet(\"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\Hand-Gestures-Recognition-master\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom_last.weights\", \"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\Hand-Gestures-Recognition-master\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\yolov3_custom.cfg\") \n",
    "\n",
    "# Load the COCO class labels\n",
    "classes = []\n",
    "with open(\"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\Hand-Gestures-Recognition-master\\\\Hand-Gestures-Recognition-master\\\\Yolo\\\\obj.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Get the names of the output layers\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Set the input image and scale it\n",
    "img = cv2.imread(\"C:\\\\Users\\\\TEJAS\\\\Downloads\\\\demoimage.png\")\n",
    "height, width, channels = img.shape\n",
    "scale = 0.00392\n",
    "blob = cv2.dnn.blobFromImage(img, scale, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "# Pass the input image through the network\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# Extract information about detected objects\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "\n",
    "# Apply non-max suppression to remove overlapping boxes\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "# Draw the final boxes on the input image\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "if len(indices) > 0:\n",
    "    for i in indices.flatten():\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = str(round(confidences[i], 2))\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
    "        cv2.putText(img, label + \" \" + confidence, (int(x), int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Show the final output image\n",
    "cv2.imshow(\"demoimage\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee396ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
